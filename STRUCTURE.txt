PROJECT STRUCTURE - VideoAgent (v1.0.0)
===============================================

VideoAgent/
├── .gitignore                    # Git ignore rules
├── .env.example                  # Environment variable template
├── pyproject.toml                # Modern Python packaging configuration
├── README.md                     # Project documentation
├── requirements.txt              # Python dependencies
├── STRUCTURE.txt                 # This file
│
├── configs/                      # Configuration files
│   ├── default.yaml              # Default experiment configuration
│   └── models.yaml               # Model pricing and capability reference
│
├── scripts/                      # Shell scripts and utilities
│   ├── run_default.sh            # Default experiment runner
│   ├── test.sh                   # Test experiment runner
│   ├── extract_invalid_videos.py # Extract failed video cases
│   └── template/
│       └── eval.sh               # Evaluation script template
│
├── video_agent/                  # Main Python package
│   ├── __init__.py               # Package initialization and public API
│   ├── cli.py                    # Command-line interface
│   ├── agent.py                  # Main VideoAgent orchestrator class
│   │
│   ├── core/                     # Core data structures
│   │   ├── __init__.py
│   │   └── video_memory.py       # VideoMemory class for frame and analysis state
│   │
│   ├── processors/               # Specialized processing modules
│   │   ├── __init__.py
│   │   ├── caption_processor.py  # CaptionProcessor for frame analysis
│   │   └── question_processor.py # QuestionProcessor for Q&A and evaluation
│   │
│   └── utils/                    # Utility modules
│       ├── __init__.py
│       ├── api.py                # AIMLClient for LLM API interactions
│       ├── config.py             # Configuration management
│       ├── cache.py              # CacheManager for response caching
│       ├── logging_utils.py      # Logging utilities
│       ├── video.py              # Video processing utilities
│       └── parsing.py            # Text and JSON parsing utilities
│
├── data/                         # Input data directory
│   └── EgoSchema_test/           # EgoSchema test dataset (499 videos)
│       ├── annotations.json      # Video annotations and questions
│       ├── video_list.txt        # List of valid video IDs
│       └── videos/               # Video files (.mp4, gitignored)
│
├── cache/                        # Performance optimization cache (gitignored)
│   ├── cache_llm.pkl             # LLM response cache
│   └── cache_clip.pkl            # CLIP embedding cache
│
└── results/                      # Experiment results (gitignored)
    └── [experiment_name]/        # Per-experiment outputs
        ├── logging.log           # Full evaluation log
        ├── result.json           # Results with Q&A + summary stats (_summary key)
        ├── metrics.csv           # Performance metrics table
        ├── summary.txt           # Human-readable summary with analysis
        ├── accuracy.txt          # Accuracy metrics (backward compatible)
        ├── experiment_config.yaml # Saved configuration
        └── videos/               # Individual video results
            └── [video_id]/
                ├── frames/       # Sampled frames as PNG files
                ├── memory.txt    # Video memory state
                ├── question.txt  # Formatted question
                └── result.json   # Video-specific results


CORE COMPONENTS
===============

1. video_agent/agent.py - Main Orchestrator
-------------------------------------------
   VideoAgent Class:
   - Coordinates entire video analysis pipeline
   - Manages experiment lifecycle and configuration
   - Handles multi-processing and result aggregation
   
   Key Methods:
   - run_experiment()              : Execute complete experiment
   - _process_video()              : Process individual video
   - _process_videos_sequential()  : Sequential video processing
   - _process_videos_multiprocessing() : Parallel video processing
   - _save_global_results()        : Generate experiment metrics

2. video_agent/core/video_memory.py - Video State
-------------------------------------------------
   VideoMemory Class:
   - Encapsulates video frames and analysis state
   - Manages sampled frame indices and captions
   - Maintains overview and event descriptions
   
   Key Methods:
   - add_sampled_frames()          : Add new frame indices
   - update_caption()              : Store frame caption
   - update_event()                : Store event description
   - update_overview()             : Update video overview
   - save_to_directory()           : Export state to disk

3. video_agent/processors/caption_processor.py - Caption Generation
-------------------------------------------------------------------
   CaptionProcessor Class:
   - Handles frame caption generation
   - Supports multi-level caption functions
   
   Key Methods:
   - generate_captions()              : Generate captions for frames
   - _write_multi_level_captions()    : Combined visual + event understanding
   - _write_group_detailed_captions() : Batch caption generation

4. video_agent/processors/question_processor.py - Q&A and Evaluation
--------------------------------------------------------------------
   QuestionProcessor Class:
   - Manages question answering and confidence evaluation
   - Handles intelligent frame resampling
   
   Key Methods:
   - answer_question()             : Generate answer from video analysis
   - evaluate_confidence()         : Assess answer confidence (1-3 scale)
   - generate_segment_steps()      : Smart frame resampling strategy

5. video_agent/utils/api.py - LLM API Interface
-----------------------------------------------
   AIMLClient Class:
   - Unified interface for text and image-based LLM interactions
   - Handles API configuration and rate limiting
   
   Key Functions:
   - get_llm_response()            : Unified LLM interface with retry logic
   - get_text_response()           : Text-only LLM requests
   - get_image_response()          : Vision-language model requests

6. video_agent/utils/config.py - Configuration
----------------------------------------------
   - load_config()                 : Load YAML configuration
   - save_config_to_output()       : Save config to output directory
   - update_config()               : Apply configuration overrides
   - Environment variable loading from .env file


WORKFLOW
========

1. Initialization
   └── VideoAgent(config)
       ├── Initialize CaptionProcessor
       ├── Initialize QuestionProcessor
       └── Setup cache and logging

2. Experiment Setup
   └── run_experiment()
       ├── Create output directory
       ├── Save configuration
       ├── Load video annotations
       └── Select processing mode (sequential/parallel)

3. Video Processing Pipeline (per video)
   └── _process_video()
       ├── Load video frames
       ├── Create VideoMemory instance
       └── Execute iterative analysis

4. Iterative Analysis (per round)
   └── While confidence < 3 and rounds < max_rounds:
       ├── Generate captions (CaptionProcessor)
       ├── Answer question (QuestionProcessor)
       ├── Evaluate confidence (QuestionProcessor)
       └── Resample frames if needed (QuestionProcessor)

5. Result Processing
   └── _save_global_results()
       ├── Calculate accuracy metrics
       ├── Save result.json
       └── Generate accuracy.txt


EVALUATION SCRIPT TEMPLATE
==========================

Location: scripts/template/eval.sh

Required Parameters:
  MODEL       - Model name/path (e.g., gpt-4o-mini-2024-07-18)
  ROUND_NAME  - Test round identifier (used for output directory)
  DATASET     - Dataset name (subset_valid, subset, test_one_video)
  COUNT       - Number of test cases (0 = full dataset)
  DETAILED    - Output verbosity (true/false)

Output Structure:
  results/<ROUND_NAME>_<MODEL>_<COUNT>_<MMDD>[_HHMM]/
    - logging.log   : Full evaluation log
    - metrics.csv   : Performance metrics table
    - summary.txt   : Human-readable summary
    - answer.json   : Detailed predictions with Q&A
    - videos/       : Per-video outputs

Usage:
  # Copy template
  cp scripts/template/eval.sh scripts/my_eval.sh
  
  # Edit parameters
  vim scripts/my_eval.sh
  
  # Run
  ./scripts/my_eval.sh


USAGE
=====

Command Line:
  python -m video_agent.cli --config default --max-videos 10

Environment Variables:
  AIML_API_KEY     - API key for LLM service
  AIML_BASE_URL    - API base URL (optional)

Configuration (configs/default.yaml):
  - scheduler_model: Model for Q&A and evaluation
  - viewer_model: Model for caption generation
  - max_rounds: Maximum analysis rounds
  - max_test_videos: Number of videos to process
  - output_dir: Results output directory

Installation:
  pip install -e .
  # or
  pip install -r requirements.txt
